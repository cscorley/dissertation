## Developer Identification {#dit-discussion}

The results outlined in the Results Section \ref{results-dit} warrants some
qualitative discussion.  In particular, our analysis shows significant affects
between snapshots and changesets, and between batch changesets and changesets
in the simulated environment.  The results are mixed between each and are not
conclusive.  However, we argue this is desirable to show that the accuracy of a
changeset-based DIT is similar to that of a snapshot-based DIT but without the
retraining cost.


### \doneq


### \dtwoq


### Situations


In this study, we've also asked two research questions which lead to two
distinct comparisons.  First, we compare a batch TM-based DIT trained on the
changesets of a project's history to one trained on the snapshot of source code
entities.  Second, we compare a batch TM-based DIT trained on changesets to a
online TM-based DIT trained on the same changesets over time.  Our results are
mixed between the research questions, hence we end up with four possible
situations; we will now discuss each of these situations in detail.


#### Batch changesets are better than batch shapshot *and* batch changesets are better than changesets in the simulated environment

#### Batch changesets are better than batch shapshot *and* changesets in the simulated environment are better than batch changesets

#### Batch snapshot are better than batch changeset *and* changesets in the simulated environment are better than batch changesets

#### Batch snapshot are better than batch changeset *and* batch changesets are better than changesets in the simulated environment

